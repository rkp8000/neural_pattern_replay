{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "\n",
    "The reactivation of neuronal activity patterns outside the context in which they originally occurred is thought to play an important role in mediating memory, but the biophysical mechanisms underlying reactivation are not well understood. Especially mysterious is the short-term replay of sequential activity patterns occurring in the recent past, since spike-timing-dependent plasticity, the biophysical mechanism normally invoked to bias model networks towards sequence production, is not thought to have a substantial effect on such short time-scales. Here we propose a model in which short-term memory for sequences is maintained in persistent activity triggered by the original sequence activation, and which directly increases the effective excitability of the neural ensembles involved in the sequence, thus leading to an increased probability of replay of sequences involving those ensembles. We show how such a phenomenon can be implemented in a simple dynamical model, and we show that the number of sequences that a randomly connected recurrent network can replay grows polynomially in the number of ensembles in the network (with degree equal to sequence length). In a simplified probabilistic model we then show how the decodability of past stimulus sequences from future neural replay sequences (in the absence of the stimulus) increases as the network connectivity becomes reflective of the stimulus transitions. Finally, we discuss the computational principals underlying our model in terms of attractors and the generation of spatiotemporal patterns from spatially defined information.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key points\n",
    "\n",
    "* Low complexity and more biophysically plausible model of sequence replay\n",
    "* Prediction that neural replay does not occur for random sequences, but rather for sequences already preferred by network; but sequences contain information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "The replay of neuronal activity patterns outside of the behavioral context in which they originally occurred is thought to play an important role in the retention and recall of memories (Gelbard-Sagiv 2008, Dupret 2010, Carr 2011). However, while neuronal pattern replay has been observed across a diverse set of brain areas, including mammalian hippocampus (Skaggs 1996, Nadasdy 1999, Louie 2001, Davison 2009), ventral striatum (Wimmer 2016), ventral tegmental area (Valdes 2015), prefrontal cortex (Euston 2007), and visual cortex (Ji 2007, Han 2008, Eagleman 2012), as well as in RA in songbirds (Dave 2000), the biophysical mechanisms underlying this phenomenon are poorly understood. This is because in order for replay to occur a neural activity pattern must be temporarily \"tagged\" upon its initial activation such that its probability of activating again at a later time is preferentially increased, but how such tagging might occur is not clear. This appears especially problematic in the replay of spatiotemporal patterns or sequences, in which not only specific sets of neurons, but directed associations between them, must be remembered and reproduced. Worthy of even further consideration is the observed replay of sequential activity patterns nearly immediately after their original activation (Han 2008, Davison 2009, Eagleman 2012), since canonical sequence-reinforcing plasticity mechanisms, such as spike-timing-dependent plasticity (STDP) (Bi 2001), are generally quite weak (Markram 1997, Bi 2001) and would not be expected to significantly alter neural network dynamics over short time courses. Indeed, network models using STDP as their primary learning rule typically require a large number of stimulus presentations (i.e., at least dozens) before the network can generate stereotyped sequences (Fiete 2010, Klampfl 2013, Huang 2015). (Some effort has been made to identify precise regimes under which STDP may have faster effects [Yger 2015], but in this case STDP did not specifically bias the generation of sequences.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alternative to mediation of short-term sequential replay by synaptic plasticity is mediation by temporary changes in network state. That is, one might imagine a portion of the network collapsing onto an *attractor* that maintains a representation from which the original sequence can later be reconstructed. Indeed, such attractor networks are perhaps the most common model for short-term memory in neural systems (Barak 2014, Chaudhuri 2016). While attractors can in general have relatively arbitrary time-varying structure yet which still contains information about previous activity patterns (Maass 2002), for the purpose of parsimony we focus on a specific subset of attractor models that have the following property: activation of an ensemble of neurons moves a portion of the network to an attractor state such that the excitability of the activated ensemble is increased while the network remains in that attractor state. We later show that neural ensembles with this property can easily be built using a standard \"rate-based\" model network (Wilson 1972) with loosely tuned connectivity, and that the attractors that emerge from sets of these ensembles are of a compositional nature, which greatly increases the set of possible stable states that can be temporarily maintained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to parsimony this choice of model class is motivated by a growing body of experimental work suggesting that neurons in many brain regions can indeed show short-term, activation-triggered increases in excitability. Specifically, it has been shown during a variety of working memory tasks that certain neurons exhibit an increased response to the second presentation of a stimulus relative to the first (reviewed in Tartaglia 2015). For example, many stimulus-selective neurons in primate inferotemporal cortex (IT) responded more strongly to a stimulus when the stimulus matched a target that had been shown a few seconds earlier, relative to when a different target had been shown (Miller 1994), with similar results later obtained in MT (Liu 2011) and V4 (Hayden 2013). A more recent experiment in which functional magnetic resonance imaging (fMRI) recordings were made on human volunteers that were shown sequences of faces found that many face-responsive voxels, which are a proxy for large populations of neurons (Huettel 2004), demonstrated a consistent enhancement of their responses to repeated faces (de Gardelle 2012). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we explore the capacity for an attractor network whose connectivity produces activation-triggered lingering hyperexcitability to demonstrate preferential replay of recent, stimulus-driven activity sequences. We first demonstrate sequential replay in a rate-based network model with a simple tree-like connectivity structure. Next we perform a graph theoretical analysis of the capacity for randomly connected recurrent networks to exhibit short-term sequential replay and show that for a fixed sequence length $L$, on average $\\sim O(N^L)$ sequences can be replayed in a network of $N$ ensembles. Using a simplified dynamical model we then show that the decodability of past stimulus sequences from future neural replay sequences increases when the internal network connectivity is reflective of the transition probabilities among stimulus elements. Finally, we discuss how our results suggest the utility and biological feasibility of large sets of stable, compositionally constructed attractor states and how the spatial information maintained in these attractors combined with the connectivity structure of the neural network can yield a rich space of replayable sequential activity patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results outline\n",
    "\n",
    "Fig 1: Proof of principle: spiking model of sequence replay. A. Heatmap of dynamical system implementation of multiple sequences being replayed that both start with same element. B. Tree-structured connectivity + zoom in on single ensemble architecture.\n",
    "\n",
    "Fig 2: Control of dynamical regime through global inputs. A. Spontaneous or stimulus-driven regime, with memories stored but not recruited. B. Spontaneous or stimulus-driven regime with memories not stored. C. Post-spontaneous or stimulus-driven, with memories recruited.\n",
    "\n",
    "Fig 3: Simplified WTA network. A. Spontaneous and triggered replay in simplified network with connectivity. B. Storage of multiple sequences and dependence on sequence overlap. C. Recruitment of hyperexcitable ensembles with weak stimuli. I.e., changing functional connectivity of network via context and changing performed computation to pattern matching in accordance with recent sequences.\n",
    "\n",
    "Fig 4: Graphical abstraction of network for capacity analysis. A. Graph diagram, with illustration of replayable path definition. B. Plots of ER capacity metrics.\n",
    "\n",
    "Fig 5: A. Simplified WTA network. Decoding and information about past stimulus from future neural replay sequence. B. Matching stimulus stats in connectivity structure gives rise to better sequence decoding. C. Learning change in stimulus statistics through one new sequence presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion points\n",
    "\n",
    "#### comparison to other mechanisms\n",
    "\n",
    "* STDP (Szatmary and Izhikevich, VelizCuba)\n",
    "* gating neurons, Conde-Sousa\n",
    "* phase-based/slow ADP (Lisman 1995, Koene 2003)\n",
    "* echo-state network trained to output delayed inputs (Jaeger 2014)\n",
    "\n",
    "#### notes about this mechanism\n",
    "\n",
    "* mechanism is not necessarily network implementation that we used, since predicts memory unit will have maxed out firing rates - other work (Crowe 2010) suggests that memory may indeed be mediated by dynamical sequences of neurons that have the same tuning; not hard to come up with simple model in which such a sequence could also increase excitability, but worth investigating how to do this without two separate populations (since growing body of evidence suggests that sensory memory can be decoded with same decoder as sensory perception)\n",
    "* we discounted STDP earlier, but there are many short-term plasticity mechanisms -- these generally are not dependent on pre- and post-synaptic activity, though, and they often also have the effect of increasing excitability in groups of connected cells, so may have similar effective functional role (+ cholinergic-modulated after depolarization (Lisman 1995))\n",
    "* repetition *suppression* also seen in data (tendency for neurons to respond more weakly to second stimulus presentation, generally thought to be because of adaptation) - thought to be indicative of predictive coding (since it's seen in tandem with enhancement, thought that prediction and prediction errors are both represented in the brain)\n",
    "* one can also consider networks whose activation depends not just on last active ensemble, but on last two or three active ensembles, by introducing delay units or timescales\n",
    "* reverse replay through specially connected networks and refractoriness (reverse replay could not be achieved even by strong short-term STDP)\n",
    "* other replay model: gamma-locked phase buffer, order indicated by different levels of hyperexcitability\n",
    "* assumption of stabilized receptive fields\n",
    "\n",
    "#### computational impliciations\n",
    "\n",
    "* one of the fundamental questions in theory of memory is how to bridge the single-neuron/synapse dynamic timescales (~ 100s of ms at max) with the timescales of synaptic plasticity (minutes to hours); this provides a way of doing through persistent activity that has high memory capacity\n",
    "* this is really an extremely simple way to have a lot of stable attractor states by way of compositionality (one of the original motivations for liquid state machines was the \"unreasonable\" number of attractors required for high memory capacity)\n",
    "    * we've mentioned that the maxed out firing rates are not necessarily accurate, though; a fascinating open question is how compositional attractor states can be achieved for more Hopfield-like attractors (Rishi's paper?)\n",
    "* also directly changes sequence probabilities, since no extra decoder is needed (as is normally needed to translate the state of an LSM, e.g., back into something meaningful)\n",
    "* also allows investigation of interaction in neural networks between spatial representations and spatiotemporal sequences\n",
    "* supports general framework for reconstruction of spatiotemporal pattern from static spatial representation and network connectivity\n",
    "* relationship to replay of experienced paths vs novel trajectories through visited areas\n",
    "\n",
    "#### relationship to other computations\n",
    "\n",
    "* predictive coding over short timescales\n",
    "* these networks, especially the random ones, cannot reproduce arbitrary sequences; however, if you allow multiple neurons to be tuned to one sequence element, can arrange architecture to get serial recall for arbitrary sequences, much like Botvinick 2007"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

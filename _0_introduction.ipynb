{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "\n",
    "A useful task for a neural network to perform is to “replay” a sequential activity pattern that it generated in the recent past. Such an ability could, for example, allow an organism to immediately recreate a reward-yielding motor sequence or to store information about a once-occurring stimulus into long-term memory by replaying the evoked neural activity sequence many times. Indeed, sequential replay is observed in in vivo neocortical dynamics. In a simple model winner-take-all network we explore how fast, activation-dependent increases in the excitability of neural ensembles could allow sequential replay to occur. Unlike the typically invoked spike-timing-dependent plasticity (STDP) learning rule, which due to its incremental effect modifies network dynamics quite slowly, excitability changes might occur much faster, due, for example, to rapid changes in local network states that persist over timescales of working memory. We show that for certain connectivity patterns such excitability changes, despite being effectively nonassociative, can bias the network towards replaying a combinatorially large number of possible sequences, and that in combination with slower learning rules, this replay can cause a novel stimulus-evoked activity sequence to become “embedded” into the network after a single stimulus presentation. Next, we analyze the replay capacity of several networks with different recurrent architectures, and we show through an information theoretic analysis that the information contained in a neural activity sequence about a past stimulus increases as the network connectivity “aligns” with the prior stimulus transition probability distribution. Finally, we discuss a general theoretical framework for interpreting our results that bridges both the working memory for and the planning of sequences. In this framework, information about a sequence is contained in steady, spatially distributed inputs to a set of ensembles, and the physical generation of the sequence arises through an interaction of this spatial input with the temporal dynamics conferred by the inter-ensemble connectivity structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Neural network dynamics can be dramatically modified by tuning the strengths of the synapses that connect the neurons in the network together. Such modifications, however, generally occur over a long timescale (i.e., minutes to days), as the synaptic learning process is relatively slow. Much less is known about how neural network dynamics might change over a short timescale (i.e., seconds), a phenomenon that is crucial for processes such as working memory. Here we consider the computational consequences of transient increases in the excitability of recently activated neural ensembles on the dynamics of a network of these ensembles. We specifically demonstrate the network's ability to replay activity sequences, despite there being no modification of the anatomical connections among the ensembles. Further, we show how such excitability-based replay can be used to embed new sequences into the network in the absence of extended training (which is typically required to embed sequences into a network). Finally, we show how our network model illuminates a general computational framework for how steady spatially distributed inputs can shape the functional connectivity structure of a network and therefore the distribution of spatiotemporal patterns that it generates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
